---
title: "Knight Lab: Local News Lens"
subtitle: |
  | Final Artifact 
authors: "Jillian Moore, Eunice Lee, Sophia Zhang, Melissa Dai and Keya Chaudhuri"
pagetitle: "Final Artifact"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

::: {.callout-tip icon="false"}
## Github Repo

[Local News Lens Repo](https://github.com/jillian-moore/knight-lab)

*Please refer to the repo's README for descriptions of each directory and R script.*
:::

# Framing Narrative
Type a sentence. 

# Collaborative Process Reflection

# Visual & Technical Documentation

#### Visual Documentation

![Image example](path/image.png){#fig-caption}

#### Technical Documentation

```{r}
#| label: qpi-scrape
#| eval: false
#| code-fold: true

# /wp-json/wp/v2/posts

# scrape the REST API ----
get_all_posts <- function(base_url, per_page = 100, max_pages = 1000, start_page = 1) {
  all_posts <- list()
  
  for(page in start_page:max_pages) {
    url <- paste0(base_url, "/wp-json/wp/v2/posts?per_page=", per_page, "&page=", page)
    
    # timeout: 20 minutes
    res <- GET(url, timeout(1200))
    
    # stop if request fails
    if(status_code(res) != 200) break
    
    posts <- fromJSON(content(res, "text", encoding = "UTF-8"), flatten = TRUE)
    
    # stop if no posts returned
    if(length(posts) == 0) break
    
    # convert all columns to character to avoid binding issues
    posts[] <- lapply(posts, as.character)
    
    all_posts[[page]] <- posts
    cat("Fetched page", page, "with", nrow(posts), "posts\n")
    
    # save point every 10 pages
    if(page %% 10 == 0) {
      temp_df <- bind_rows(all_posts)
      saveRDS(temp_df, here("data", paste0("checkpoint_page_", page, ".rds")))
      cat("Saved checkpoint at page", page, "\n")
    }
  }
  
  bind_rows(all_posts)
}

# call fxn ----
all_posts_df <- get_all_posts("https://blockclubchicago.org")
saveRDS(all_posts_df, here("data", "api_scrape.rds"))
```

```{r}
#| label: actual-mappings
#| eval: false
#| code-fold: true

# flattened neighborhood mapping
neighborhood_mapping <- tribble(
  ~sub_community, ~chi_community,
  
  # Citywide
  "Citywide", "Chicago",
  "Chicago", "Chicago",
  
  # West Side
  "Austin", "Austin",
  "Austin", "West Garfield Park",
  "Austin", "East Garfield Park",
  "Broadview", "Austin",
  "Garfield Park", "West Garfield Park",
  "Garfield Park", "East Garfield Park",
  "North Lawndale", "North Lawndale",
  "South Lawndale", "South Lawndale",
  "Little Village", "South Lawndale",
  "West Lawn", "West Lawn",
  "West Loop", "Near West Side",
  "Near West Side", "Near West Side",
  "Humboldt Park", "Humboldt Park",
  "West Town", "West Town",
  "Wicker Park", "West Town",
  "Bucktown", "West Town",
  "Lower West Side", "Lower West Side",
  "Pilsen", "Lower West Side",
  
  # North Side
  "Rogers Park", "Rogers Park",
  "Edgewater", "Edgewater",
  "Uptown", "Uptown",
  "Bowmanville", "Lincoln Square",
  "Lincoln Square", "Lincoln Square",
  "Andersonville", "Lincoln Square",
  "Ravenswood", "Lincoln Square",
  "North Center", "North Center",
  "Ravenswood", "North Center",
  "Roscoe Village", "North Center",
  "Lake View", "Lake View",
  "Lakeview", "Lake View",
  "Wrigleyville", "Lake View",
  "Northalsted", "Lake View",
  "Lincoln Park", "Lincoln Park",
  "Old Town", "Lincoln Park",
  "Near North Side", "Near North Side",
  "River North", "Near North Side",
  "Gold Coast", "Near North Side",
  "Edison Park", "Edison Park",
  "Norwood Park", "Norwood Park",
  "Jefferson Park", "Jefferson Park",
  "Forest Glen", "Forest Glen",
  "North Park", "North Park",
  "Albany Park", "Albany Park",
  "Portage Park", "Portage Park",
  "Irving Park", "Irving Park",
  "Dunning", "Dunning",
  "Montclare", "Montclare",
  "Belmont Cragin", "Belmont Cragin",
  "Hermosa", "Hermosa",
  "Avondale", "Avondale",
  "Logan Square", "Logan Square",
  
  # Northwest Side
  "O'Hare", "O'Hare",
  "Edgebrook", "Edgebrook",
  "Sauganash", "Sauganash",
  
  # Central
  "Loop", "Loop",
  "City Hall", "Loop",
  "Downtown", "Loop",
  "South Loop", "Loop",
  "Near South Side", "Near South Side",
  "South Loop", "Near South Side",
  "Armour Square", "Armour Square",
  "Chinatown", "Armour Square",
  "Douglas", "Douglas",
  "Oakland", "Oakland",
  "Fuller Park", "Fuller Park",
  "Grand Boulevard", "Grand Boulevard",
  "Bronzeville", "Grand Boulevard",
  "Kenwood", "Kenwood",
  "Bronzeville", "Kenwood",
  "Washington Park", "Washington Park",
  "Bronzeville", "Washington Park",
  "Hyde Park", "Hyde Park",
  "Woodlawn", "Woodlawn",
  
  # South Side
  "Bridgeport", "Bridgeport",
  "McKinley Park", "McKinley Park",
  "Brighton Park", "Brighton Park",
  "Archer Heights", "Archer Heights",
  "Gage Park", "Gage Park",
  "Clearing", "Clearing",
  "West Elsdon", "West Elsdon",
  "Garfield Ridge", "Garfield Ridge",
  "Back of the Yards", "New City",
  "West Englewood", "West Englewood",
  "Englewood", "Englewood",
  "Greater Grand Crossing", "Greater Grand Crossing",
  "Ashburn", "Ashburn",
  "Auburn Gresham", "Auburn Gresham",
  "Beverly", "Beverly",
  "Washington Heights", "Washington Heights",
  "Mount Greenwood", "Mount Greenwood",
  "Morgan Park", "Morgan Park",
  "Chatham", "Chatham",
  "Avalon Park", "Avalon Park",
  "South Shore", "South Shore",
  "South Chicago", "South Chicago",
  "Burnside", "Burnside",
  "Calumet Heights", "Calumet Heights",
  "Roseland", "Roseland",
  "Pullman", "Pullman",
  "South Deering", "South Deering",
  "East Side", "East Side",
  "West Pullman", "West Pullman",
  "Riverdale", "Riverdale",
  "Hegewisch", "Hegewisch"
) |>
  mutate(
    sub_community = str_to_lower(str_trim(sub_community)), 
    chi_community = str_to_lower(str_trim(chi_community))  
  )

chi_communities <- c(
  "Rogers Park", "West Ridge", "Uptown", "Lincoln Square", 
  "North Center", "Lake View", "Lincoln Park", "Near North Side", 
  "Edison Park", "Norwood Park", "Jefferson Park", "Forest Glen", 
  "North Park", "Albany Park", "Portage Park", "Irving Park", 
  "Dunning", "Montclare", "Belmont Cragin", "Hermosa", 
  "Avondale", "Logan Square", "Humboldt Park", "West Town", 
  "Austin", "West Garfield Park", "East Garfield Park", "Near West Side",
  "North Lawndale", "South Lawndale", "Lower West Side", "Loop", 
  "Near South Side", "Armour Square", "Douglas", "Oakland", 
  "Fuller Park", "Grand Boulevard", "Kenwood", "Washington Park", 
  "Hyde Park", "Woodlawn", "South Shore", "Chatham", "Avalon Park",
  "South Chicago", "Burnside", "Calumet Heights", "Roseland", 
  "Pullman", "South Deering", "East Side", "West Pullman", 
  "Riverdale", "Hegewisch", "Garfield Ridge", "Archer Heights", 
  "Brighton Park", "McKinley Park", "Bridgeport", "New City", 
  "West Elsdon", "Gage Park", "Clearing", "West Lawn", "Chicago Lawn", 
  "West Englewood", "Englewood", "Greater Grand Crossing", 
  "Ashburn", "Auburn Gresham", "Beverly", "Washington Heights", 
  "Mount Greenwood", "Morgan Park", "Jeffery Manor", "East Morgan Park", 
  "West Morgan Park"
)
```

```{r}
#| label: location-fxn
#| eval: false
#| code-fold: true

# match neighborhoods row by row using our mapping
api_clean <- api_clean |>
  rowwise() |>
  mutate(
    sub_community = str_to_lower(str_trim(sub_community)),
    article_section_split = str_split(parsely.meta.articleSection, ",\\s*"),
    primary_category_split = str_split(slp_primary_category.name, ",\\s*")
  ) |>
  rowwise() |>
  mutate(
    matched_neighborhoods = list({
      # empty vector to store matched neighborhoods
      matches <- character()
      
      # does sub_community field match a neighborhood?
      if (!is.na(sub_community) && sub_community != "") {
        current_sub <- sub_community
        
        # look up sub_community in neighborhood_mapping
        sub_matches <- neighborhood_mapping |>
          filter(sub_community == current_sub | chi_community == current_sub) |>
          pull(chi_community) |>
          unique()
        
        # if matches found, ONLY use these
        if (length(sub_matches) > 0) {
          matches <- sub_matches
        }
      }
      
      # if no sub_community match, check ALL items in article sections
      if (length(matches) == 0 && length(article_section_split) > 0 && !is.null(article_section_split[[1]])) {
        sections_clean <- str_to_lower(str_trim(article_section_split[[1]]))
        sections_clean <- sections_clean[!is.na(sections_clean) & sections_clean != ""]
        
        # loop through EACH section and collect ALL matches
        for (section in sections_clean) {
          section_matches <- neighborhood_mapping |>
            filter(sub_community == section | chi_community == section) |>
            pull(chi_community) |>
            unique()
          matches <- c(matches, section_matches)
        }
      }
      
      # if still no matches, check ALL items in primary category
      if (length(matches) == 0 && length(primary_category_split) > 0 && !is.null(primary_category_split[[1]])) {
        categories_clean <- str_to_lower(str_trim(primary_category_split[[1]]))
        categories_clean <- categories_clean[!is.na(categories_clean) & categories_clean != ""]
        
        # loop through EACH category and collect ALL matches
        for (category in categories_clean) {
          category_matches <- neighborhood_mapping |>
            filter(sub_community == category | chi_community == category) |>
            pull(chi_community) |>
            unique()
          matches <- c(matches, category_matches)
        }
      }
      
      # return all unique matches found
      unique(matches)
    })
  ) |>
  ungroup()

# add separate neighborhood columns
api_clean <- api_clean |>
  mutate(
    neighborhood1 = map_chr(matched_neighborhoods, ~{if(length(.x) >= 1) .x[1] else NA_character_}),
    neighborhood2 = map_chr(matched_neighborhoods, ~{if(length(.x) >= 2) .x[2] else NA_character_}),
    neighborhood3 = map_chr(matched_neighborhoods, ~{if(length(.x) >= 3) .x[3] else NA_character_})
  ) |> 
  mutate(neighborhood1 = replace_na(neighborhood1, "chicago"))
```

```{r}
#| label: merge
#| code-fold: true
#| eval: false

# merge files ----
# shapefile and census merge (77 neighborhoods only)
chi_boundaries_clean <- chi_boundaries |> 
  select(community, the_geom, area_num_1, shape_area, shape_len) |> 
  left_join(census_clean, by = "community") |> 
  mutate(community = str_to_lower(community))

# pivot api longer and prepare date
api_long <- api_clean |> 
  mutate(
    date = as.Date(date),
    year = year(date),
    month = month(date),
    year_month = floor_date(date, "month")
  ) |> 
  pivot_longer(
    cols = starts_with("neighborhood"),
    names_to = "neighborhood_col",
    values_to = "community"
  ) |> 
  filter(!is.na(community) & community != "") |> 
  select(-neighborhood_col)

# keep individual article rows with dates (includes BOTH neighborhood and citywide)
api_detail <- api_long |> 
  select(community, topic_tag, topic_confidence, date, year, month, year_month)

# get date range for slider
date_range <- api_long |> 
  summarise(
    min_date = min(date, na.rm = TRUE),
    max_date = max(date, na.rm = TRUE)
  )

# aggregate api data by community, topic, and date
api_summary <- api_long |> 
  group_by(community, topic_tag, date) |> 
  summarise(article_count = n(), .groups = "drop")

# get total articles by community with date info
api_community_totals <- api_long |> 
  group_by(community) |> 
  summarise(
    total_articles = n(),
    .groups = "drop"
  )

# create aggregated "chicago" row with total population from all neighborhoods
chicago_totals <- census_clean |> 
  summarise(
    community = "chicago",
    total_population = sum(total_population, na.rm = TRUE),
    white = sum(white, na.rm = TRUE),
    black_or_african_american = sum(black_or_african_american, na.rm = TRUE),
    american_indian_or_alaska_native = sum(american_indian_or_alaska_native, na.rm = TRUE),
    asian = sum(asian, na.rm = TRUE),
    native_hawaiian_or_pacific_islander = sum(native_hawaiian_or_pacific_islander, na.rm = TRUE),
    other_race = sum(other_race, na.rm = TRUE),
    multiracial = sum(multiracial, na.rm = TRUE),
    hispanic_or_latino = sum(hispanic_or_latino, na.rm = TRUE),
    white_not_hispanic_or_latino = sum(white_not_hispanic_or_latino, na.rm = TRUE),
    under_25_000 = sum(under_25_000, na.rm = TRUE),
    x25_000_to_49_999 = sum(x25_000_to_49_999, na.rm = TRUE),
    x50_000_to_74_999 = sum(x50_000_to_74_999, na.rm = TRUE),
    x75_000_to_125_000 = sum(x75_000_to_125_000, na.rm = TRUE),
    x125_000 = sum(x125_000, na.rm = TRUE),
    age_0_17 = sum(age_0_17, na.rm = TRUE),
    age_18_24 = sum(age_18_24, na.rm = TRUE),
    age_25_34 = sum(age_25_34, na.rm = TRUE),
    age_35_49 = sum(age_35_49, na.rm = TRUE),
    age_50_64 = sum(age_50_64, na.rm = TRUE),
    age_65_plus = sum(age_65_plus, na.rm = TRUE),
    # Placeholder values for spatial columns
    the_geom = NA_character_,
    area_num_1 = NA_real_,
    shape_area = NA_real_,
    shape_len = NA_real_,
    acs_year = NA_real_
  )

# merge with spatial data (77 neighborhoods only, then add chicago row)
full_data_neighborhoods <- chi_boundaries_clean |> 
  left_join(
    api_community_totals |> filter(community != "chicago"), 
    by = "community"
  ) |> 
  left_join(
    api_summary |> filter(community != "chicago"), 
    by = "community", 
    relationship = "many-to-many"
  )

# merge citywide data
full_data_citywide <- chicago_totals |>
  left_join(
    api_community_totals |> filter(community == "chicago"),
    by = "community"
  ) |>
  left_join(
    api_summary |> filter(community == "chicago"),
    by = "community",
    relationship = "many-to-many"
  )

# combine neighborhoods and citywide
full_data <- bind_rows(full_data_neighborhoods, full_data_citywide)

# clean up and handle NAs
full_data <- full_data |> 
  select(-starts_with("female"), -starts_with("male")) |> 
  select(-ends_with(".y")) |> 
  rename_with(~str_remove(., "\\.x$"), ends_with(".x")) |> 
  mutate(
    total_articles = replace_na(total_articles, 0),
    article_count = replace_na(article_count, 0),
    topic_tag = replace_na(topic_tag, "No Coverage")
  )

# add per person calculations
full_data <- full_data |>
  mutate(
    # overall articles per person
    articles_per_person = if_else(total_population > 0, 
                                  total_articles / total_population, 
                                  0),
    articles_per_1000 = if_else(total_population > 0, 
                                (total_articles / total_population) * 1000, 
                                0),
    
    # articles per 1,000 by AGE
    topic_articles_per_0_17 = if_else(age_0_17 > 0, 
                                      article_count / age_0_17, 
                                      0),
    topic_articles_per_18_24 = if_else(age_18_24 > 0, 
                                       article_count / age_18_24, 
                                       0),
    topic_articles_per_25_34 = if_else(age_25_34 > 0, 
                                       article_count / age_25_34, 
                                       0),
    topic_articles_per_35_49 = if_else(age_35_49 > 0, 
                                       article_count / age_35_49, 
                                       0),
    topic_articles_per_50_64 = if_else(age_50_64 > 0, 
                                       article_count / age_50_64, 
                                       0),
    topic_articles_per_65_plus = if_else(age_65_plus > 0, 
                                         article_count / age_65_plus, 
                                         0),
    
    # articles per 1,000 by RACE/ETHNICITY
    articles_per_white = if_else(white > 0, 
                                 article_count / white * 1000, 
                                 0),
    articles_per_black = if_else(black_or_african_american > 0, 
                                 article_count / black_or_african_american * 1000, 
                                 0),
    articles_per_asian = if_else(asian > 0, 
                                 article_count / asian * 1000, 
                                 0),
    articles_per_native_american = if_else(american_indian_or_alaska_native > 0, 
                                           article_count / american_indian_or_alaska_native * 1000, 
                                           0),
    articles_per_pacific_islander = if_else(native_hawaiian_or_pacific_islander > 0, 
                                            article_count / native_hawaiian_or_pacific_islander * 1000, 
                                            0),
    articles_per_other_race = if_else(other_race > 0, 
                                      article_count / other_race * 1000, 
                                      0),
    articles_per_multiracial = if_else(multiracial > 0, 
                                       article_count / multiracial * 1000, 
                                       0),
    articles_per_hispanic = if_else(hispanic_or_latino > 0, 
                                    article_count / hispanic_or_latino * 1000, 
                                    0),
    articles_per_white_non_hispanic = if_else(white_not_hispanic_or_latino > 0, 
                                              article_count / white_not_hispanic_or_latino * 1000, 
                                              0),
    
    # articles per 1,000 by INCOME bracket
    articles_per_under_25k = if_else(under_25_000 > 0, 
                                     article_count / under_25_000 * 1000, 
                                     0),
    articles_per_25k_to_50k = if_else(x25_000_to_49_999 > 0, 
                                      article_count / x25_000_to_49_999 * 1000, 
                                      0),
    articles_per_50k_to_75k = if_else(x50_000_to_74_999 > 0, 
                                      article_count / x50_000_to_74_999 * 1000, 
                                      0),
    articles_per_75k_to_125k = if_else(x75_000_to_125_000 > 0, 
                                       article_count / x75_000_to_125_000 * 1000, 
                                       0),
    articles_per_over_125k = if_else(x125_000 > 0, 
                                     article_count / x125_000 * 1000, 
                                     0)
  )

# prepare data for shiny app ----
# create chi_boundaries_sf (spatial data with census info - 77 NEIGHBORHOODS ONLY, no chicago)
chi_boundaries_sf <- chi_boundaries_clean |> 
  st_as_sf(wkt = "the_geom", crs = 4326) |> 
  select(community, total_population, white, black_or_african_american, 
         american_indian_or_alaska_native, asian, native_hawaiian_or_pacific_islander,
         other_race, multiracial, hispanic_or_latino,
         under_25_000, x25_000_to_49_999, x50_000_to_74_999, 
         x75_000_to_125_000, x125_000,
         age_0_17, age_18_24, age_25_34, age_35_49, age_50_64, age_65_plus,
         the_geom) |> 
  distinct(community, .keep_all = TRUE)

# create article_data (includes BOTH neighborhood AND citywide articles)
article_data <- api_detail |> 
  rename(
    article_date = date,
    topic_match = topic_tag
  ) |> 
  select(community, topic_match, article_date, year, month, year_month)

# create topics vector
topic_choices <- c(
  "Arts & Culture", "Business", "Crime & Public Safety", "Education",
  "Food & Restaurants", "Health & Environment", "Housing", "Immigration",
  "Politics", "Social Movements", "Sports & Recreation", "Transportation & Infrastructure"
)

# name demographic vars
demo_choices <- c(
  "None" = "None",
  "Total Population" = "total_population",
  "White" = "white",
  "Black/African American" = "black_or_african_american",
  "Asian" = "asian",
  "Native American" = "american_indian_or_alaska_native",
  "Pacific Islander" = "native_hawaiian_or_pacific_islander",
  "Other Race" = "other_race",
  "Multiracial" = "multiracial",
  "Hispanic/Latino" = "hispanic_or_latino",
  "Income: Under $25k" = "under_25_000",
  "Income: $25k-$50k" = "x25_000_to_49_999",
  "Income: $50k-$75k" = "x50_000_to_74_999",
  "Income: $75k-$125k" = "x75_000_to_125_000",
  "Income: Over $125k" = "x125_000",
  "Age: 0-17" = "age_0_17",
  "Age: 18-24" = "age_18_24",
  "Age: 25-34" = "age_25_34",
  "Age: 35-49" = "age_35_49",
  "Age: 50-64" = "age_50_64",
  "Age: 65+" = "age_65_plus"
)

# convert date_range to list format
date_range <- list(
  min_date = date_range$min_date,
  max_date = date_range$max_date
)

# save out ----
save(
  full_data,
  article_data, 
  chi_boundaries_sf,      # only 77 neighborhoods with spatial boundaries
  article_data,           # aLL articles including "chicago" citywide
  date_range,             # date range for slider
  topic_choices,          # topic options
  demo_choices,           # demographic variable options
  file = here("data/full_data.rda")
)

save(api_clean, file = here("data/ai_check_data.rda"))
```

```{r}
#| label: ai-tags
#| code-fold: true
#| eval: false

`%||%` <- function(x, y) if (is.null(x)) y else x

# constants ----
openai_api_key <- Sys.getenv("OPEN_AI_KEY")

valid_topics <- c(
  "Arts & Culture", "Business", "Crime & Public Safety", "Education",
  "Food & Restaurants", "Health & Environment", "Housing", "Immigration",
  "Politics", "Social Movements", "Sports & Recreation", "Transportation & Infrastructure"
)

# helper functions ----
clean_html_content <- function(html_text) {
  tryCatch({
    cleaned <- read_html(html_text) %>% html_text2()
    cleaned <- str_squish(cleaned)
    substr(cleaned, 1, 2000)
  }, error = function(e) {
    cleaned <- gsub("<[^>]+>", " ", html_text)
    cleaned <- str_squish(cleaned)
    substr(cleaned, 1, 2000)
  })
}

classify_article <- function(text, api_key, max_retries = 5) {
  clean_text <- clean_html_content(text)
  topic_list <- paste("-", valid_topics, collapse = "\n")
  prompt <- paste(
    "classify this chicago news article into one of these topics:",
    topic_list,
    "\nreturn only json:",
    '{"topic": "Topic Name", "confidence": 0.85}',
    "\n--- article start ---\n",
    clean_text,
    "\n--- article end ---"
  )
  
  for (attempt in 1:max_retries) {
    result <- tryCatch({
      resp <- request("https://api.openai.com/v1/chat/completions") %>%
        req_headers(Authorization = paste("Bearer", api_key),
                    `Content-Type` = "application/json") %>%
        req_body_json(list(
          model = "gpt-4o-mini",
          messages = list(list(role = "user", content = prompt)),
          temperature = 0.5
        )) %>%
        req_timeout(60) %>%
        req_perform() %>%
        resp_body_json()
      
      raw <- resp$choices[[1]]$message$content
      json_str <- str_extract(raw, "\\{[\\s\\S]*\\}")
      parsed <- fromJSON(json_str)
      topic <- parsed$topic %||% NA_character_
      if (!topic %in% valid_topics) topic <- NA_character_
      
      list(topic = topic, confidence = as.numeric(parsed$confidence %||% NA_real_), success = TRUE)
    }, error = function(e) {
      NULL
    })
    
    if (!is.null(result)) return(result)
    Sys.sleep(2)
  }
  list(topic = NA_character_, confidence = NA_real_, success = FALSE)
}

# auto-restart processing ----
process_articles <- function(articles_file = "data/api_scrape.rds",
                             checkpoint_file = "data/my_openai_checkpoint.rds",
                             batch_size = 10,
                             pause_minutes = 1) {
  
  all_articles <- readRDS(articles_file) %>%
    mutate(row_id = row_number()) %>%
    select(id, row_id, content.rendered)
  
  results <- if (file.exists(checkpoint_file)) readRDS(checkpoint_file) else tibble(
    id = character(), row_id = integer(), content.rendered = character(),
    topic_tag = character(), topic_confidence = numeric()
  )
  
  remaining <- anti_join(all_articles, results, by = "row_id")
  total <- nrow(all_articles)
  
  while (nrow(remaining) > 0) {
    for (i in seq_len(nrow(remaining))) {
      art <- remaining[i, ]
      cat("\nfetching article", art$row_id, "of", total, "\n")
      
      res <- classify_article(art$content.rendered, openai_api_key)
      results <- bind_rows(results, tibble(
        id = art$id, row_id = art$row_id, content.rendered = art$content.rendered,
        topic_tag = res$topic, topic_confidence = res$confidence
      ))
      
      if (i %% batch_size == 0 || i == nrow(remaining)) {
        saveRDS(results, checkpoint_file)
        cat("âœ“ checkpoint saved (", nrow(results), "total)\n")
      }
      Sys.sleep(0.5)
    }
    
    remaining <- anti_join(all_articles, results, by = "row_id")
    if (nrow(remaining) > 0) {
      cat("\npausing for", pause_minutes, "minute(s) to avoid rate limits...\n")
      Sys.sleep(pause_minutes * 60)
    }
  }
  cat("\nðŸŽ‰ all articles processed!\n")
  results
}

# call function ----
final_results <- process_articles(
  articles_file = "data/api_scrape.rds",
  checkpoint_file = "data/my_openai_checkpoint.rds",
  batch_size = 10,
  pause_minutes = 1
)

saveRDS(final_results, here("data", "articles_with_topics_final.rds"))
```

```{r}
#| label: map-color
#| code-fold: true
#| eval: false

# chunked from modules directory

      # calculate display value
      if (input$metric_type == "per_capita") {
        map_data <- map_data %>%
          mutate(display_value = if_else(total_population > 0, 
                                         (article_count / total_population) * 1000, 
                                         0))
      } else {
        map_data <- map_data %>%
          mutate(display_value = article_count)
      }
      
      n <- nrow(map_data)
      
      # blue intensity (articles) - Using complementary cyan/blue
      blue_intensity <- rep(0, n)
      if (input$blue_var != "None") {
        max_val <- max(map_data$display_value, na.rm = TRUE)
        if (!is.infinite(max_val) && max_val > 0) {
          normalized_vals <- map_data$display_value / max_val
          blue_intensity <- sqrt(normalized_vals)
        }
      }
      
      # yellow intensity (demographics) - Using tertiary yellow
      yellow_intensity <- rep(0, n)
      if (input$demo_var != "None" && input$demo_var %in% names(map_data)) {
        demo_vals <- suppressWarnings(as.numeric(map_data[[input$demo_var]]))
        demo_vals[is.na(demo_vals)] <- 0
        max_demo <- max(demo_vals, na.rm = TRUE)
        if (!is.infinite(max_demo) && max_demo > 0) {
          normalized_demo <- demo_vals / max_demo
          yellow_intensity <- sqrt(normalized_demo)
        }
      }
      
      # determine colors - Orange Line palette
      fill_colors <- rep("#f1f3f2", n)  # color-tertiary-light for no data
      
      # CITYWIDE TOGGLE ONLY - show blue when everything else is "None"
      if (input$blue_var == "None" && input$demo_var == "None" && isTRUE(input$include_citywide)) {
        # Blue gradient for citywide-only view
        citywide_max <- max(map_data$article_count, na.rm = TRUE)
        if (!is.infinite(citywide_max) && citywide_max > 0) {
          fill_colors <- sapply(1:n, function(i) {
            if (map_data$article_count[i] == 0) return("#f1f3f2")
            intensity <- sqrt(map_data$article_count[i] / citywide_max)
            # from light cyan to dark teal
            r <- round(173 + (0 - 173) * intensity)
            g <- round(216 + (121 - 216) * intensity)
            b <- round(230 + (147 - 230) * intensity)
            rgb(r, g, b, maxColorValue = 255)
          })
        }
      } else if (input$blue_var != "None" && (input$demo_var == "None")) {
        # blue gradient: complementary light to dark
        fill_colors <- sapply(1:n, function(i) {
          intensity <- blue_intensity[i]
          if (intensity == 0) return("#f1f3f2")
          # from light cyan to dark teal (complementary to orange)
          r <- round(173 + (0 - 173) * intensity)
          g <- round(216 + (121 - 216) * intensity)
          b <- round(230 + (147 - 230) * intensity)
          rgb(r, g, b, maxColorValue = 255)
        })
      } else if (input$blue_var == "None" && input$demo_var != "None") {
        # yellow gradient: tertiary light to dark
        fill_colors <- sapply(1:n, function(i) {
          intensity <- yellow_intensity[i]
          if (intensity == 0) return("#f1f3f2")
          # From #f1ece4 to #eec200 (tertiary colors)
          r <- round(241 + (238 - 241) * intensity)
          g <- round(236 + (194 - 236) * intensity)
          b <- round(228 + (0 - 228) * intensity)
          rgb(r, g, b, maxColorValue = 255)
        })
      } else if (input$blue_var != "None" && input$demo_var != "None") {
        # green blend when both selected (secondary color)
        fill_colors <- sapply(1:n, function(i) {
          blue_val <- blue_intensity[i]
          yellow_val <- yellow_intensity[i]
          if(blue_val == 0 && yellow_val == 0) return("#f1f3f2")
          
          avg_intensity <- (blue_val + yellow_val) / 2
          total <- blue_val + yellow_val
          blue_weight <- blue_val / total
          
          if(blue_weight > 0.6) {
            # more blue - cyan tones
            r <- round(173 + (0 - 173) * avg_intensity)
            g <- round(216 + (150 - 216) * avg_intensity)
            b <- round(230 + (136 - 230) * avg_intensity)
          } else if(blue_weight < 0.4) {
            # more yellow - warm green
            r <- round(200 + (139 - 200) * avg_intensity)
            g <- round(220 + (195 - 220) * avg_intensity)
            b <- round(150 + (74 - 150) * avg_intensity)
          } else {
            # balanced - secondary green (#00bf7d)
            r <- round(180 + (0 - 180) * avg_intensity)
            g <- round(220 + (191 - 220) * avg_intensity)
            b <- round(200 + (125 - 200) * avg_intensity)
          }
          rgb(r, g, b, maxColorValue = 255)
        })
      }
```

# Reflective Analysis

# Design & Presentation
